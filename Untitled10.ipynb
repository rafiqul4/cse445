{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5bc454-f5f7-4411-a782-b1bbffe5b11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_17952\\1609340602.py:145: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\b\\abs_fakvb73nko\\croot\\pytorch-select_1730848725921\\work\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  states = torch.FloatTensor(states).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500 | W: 287 | L: 137 | D: 76\n",
      "Episode 1000 | W: 276 | L: 145 | D: 79\n",
      "Episode 1500 | W: 278 | L: 122 | D: 100\n",
      "Episode 2000 | W: 279 | L: 120 | D: 101\n",
      "Episode 2500 | W: 259 | L: 140 | D: 101\n",
      "Episode 3000 | W: 246 | L: 121 | D: 133\n",
      "Episode 3500 | W: 255 | L: 140 | D: 105\n",
      "Episode 4000 | W: 242 | L: 118 | D: 140\n",
      "Episode 4500 | W: 232 | L: 127 | D: 141\n",
      "Episode 5000 | W: 285 | L: 106 | D: 109\n",
      "Episode 5500 | W: 260 | L: 102 | D: 138\n",
      "Episode 6000 | W: 271 | L: 114 | D: 115\n",
      "Episode 6500 | W: 276 | L: 85 | D: 139\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "EPISODES = 15000\n",
    "GAMMA = 0.95\n",
    "EPSILON = 1.0\n",
    "EPSILON_DECAY = 0.9995\n",
    "EPSILON_MIN = 0.05\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 64\n",
    "MEMORY_SIZE = 10000\n",
    "\n",
    "# Track training stats\n",
    "losses = []\n",
    "win_rates = []\n",
    "draw_rates = []\n",
    "loss_rates = []\n",
    "epsilons = []\n",
    "avg_q_values = []\n",
    "episode_rewards = []\n",
    "rolling_win = []\n",
    "rolling_draw = []\n",
    "rolling_loss = []\n",
    "\n",
    "# --- Tic Tac Toe Environment ---\n",
    "class TicTacToeEnv:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros(9, dtype=int)\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        return self.board.copy()\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        return [i for i in range(9) if self.board[i] == 0]\n",
    "\n",
    "    def check_winner(self, board):\n",
    "        wins = [(0,1,2), (3,4,5), (6,7,8),\n",
    "                (0,3,6), (1,4,7), (2,5,8),\n",
    "                (0,4,8), (2,4,6)]\n",
    "        for i, j, k in wins:\n",
    "            total = board[i] + board[j] + board[k]\n",
    "            if total == 3:\n",
    "                return 1\n",
    "            elif total == -3:\n",
    "                return -1\n",
    "        return 0\n",
    "\n",
    "    def step(self, action, player=1):\n",
    "        if self.board[action] != 0 or self.done:\n",
    "            return self.board.copy(), -10, True\n",
    "        self.board[action] = player\n",
    "        winner = self.check_winner(self.board)\n",
    "        if winner != 0:\n",
    "            self.done = True\n",
    "            self.winner = winner\n",
    "            return self.board.copy(), 1 if winner == 1 else -1, True\n",
    "        if 0 not in self.board:\n",
    "            self.done = True\n",
    "            self.winner = 0\n",
    "            return self.board.copy(), 0.5, True\n",
    "        return self.board.copy(), 0, False\n",
    "\n",
    "# --- DQN Model ---\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(9, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 9)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- Replay Memory ---\n",
    "class ReplayMemory:\n",
    "    def __init__(self, size):\n",
    "        self.memory = deque(maxlen=size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "\n",
    "# --- Training Function ---\n",
    "def train_dqn():\n",
    "    global EPSILON\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    env = TicTacToeEnv()\n",
    "    model = DQN().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.MSELoss()\n",
    "    memory = ReplayMemory(MEMORY_SIZE)\n",
    "    stats = {\"win\": 0, \"draw\": 0, \"loss\": 0}\n",
    "\n",
    "    for episode in range(1, EPISODES + 1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        q_total = 0\n",
    "        q_count = 0\n",
    "\n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            if random.random() < EPSILON:\n",
    "                action = random.choice(env.get_valid_actions())\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = model(state_tensor)\n",
    "                    valid_q = q_values[0][env.get_valid_actions()]\n",
    "                    action = env.get_valid_actions()[valid_q.argmax().item()]\n",
    "                    q_total += valid_q.max().item()\n",
    "                    q_count += 1\n",
    "\n",
    "            next_state, reward, done = env.step(action, player=1)\n",
    "            total_reward += reward\n",
    "\n",
    "            if not done:\n",
    "                opp_action = random.choice(env.get_valid_actions())\n",
    "                next_state, opp_reward, done = env.step(opp_action, player=-1)\n",
    "                if opp_reward == -1:\n",
    "                    reward = -1\n",
    "                total_reward += opp_reward\n",
    "\n",
    "            memory.add((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "\n",
    "            # Training step\n",
    "            batch = memory.sample(BATCH_SIZE)\n",
    "            if len(batch) > 1:\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                states = torch.FloatTensor(states).to(device)\n",
    "                next_states = torch.FloatTensor(next_states).to(device)\n",
    "                actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "                rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "                dones = torch.BoolTensor(dones).unsqueeze(1).to(device)\n",
    "\n",
    "                q_values = model(states).gather(1, actions)\n",
    "                with torch.no_grad():\n",
    "                    max_next_q = model(next_states).max(1)[0].unsqueeze(1)\n",
    "                    target_q = rewards + GAMMA * max_next_q * (~dones)\n",
    "\n",
    "                loss = criterion(q_values, target_q)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        if EPSILON > EPSILON_MIN:\n",
    "            EPSILON *= EPSILON_DECAY\n",
    "\n",
    "        epsilons.append(EPSILON)\n",
    "        avg_q_values.append(q_total / q_count if q_count > 0 else 0)\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        if env.winner == 1:\n",
    "            stats[\"win\"] += 1\n",
    "        elif env.winner == -1:\n",
    "            stats[\"loss\"] += 1\n",
    "        else:\n",
    "            stats[\"draw\"] += 1\n",
    "\n",
    "        if episode % 500 == 0:\n",
    "            total = stats[\"win\"] + stats[\"draw\"] + stats[\"loss\"]\n",
    "            win_rates.append(stats[\"win\"] / total)\n",
    "            draw_rates.append(stats[\"draw\"] / total)\n",
    "            loss_rates.append(stats[\"loss\"] / total)\n",
    "            rolling_win.append(stats[\"win\"])\n",
    "            rolling_draw.append(stats[\"draw\"])\n",
    "            rolling_loss.append(stats[\"loss\"])\n",
    "            print(f\"Episode {episode} | W: {stats['win']} | L: {stats['loss']} | D: {stats['draw']}\")\n",
    "            stats = {\"win\": 0, \"draw\": 0, \"loss\": 0}\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Plotting Function ---\n",
    "def plot_metrics():\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(15, 10))\n",
    "\n",
    "    axs[0, 0].plot(losses)\n",
    "    axs[0, 0].set_title(\"Training Loss\")\n",
    "\n",
    "    axs[0, 1].plot(win_rates, label=\"Win\")\n",
    "    axs[0, 1].plot(draw_rates, label=\"Draw\")\n",
    "    axs[0, 1].plot(loss_rates, label=\"Loss\")\n",
    "    axs[0, 1].set_title(\"Win/Draw/Loss Rates\")\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    axs[1, 0].plot(epsilons)\n",
    "    axs[1, 0].set_title(\"Epsilon Decay\")\n",
    "\n",
    "    axs[1, 1].plot(avg_q_values)\n",
    "    axs[1, 1].set_title(\"Average Q-values\")\n",
    "\n",
    "    axs[2, 0].plot(episode_rewards)\n",
    "    axs[2, 0].set_title(\"Reward per Episode\")\n",
    "\n",
    "    axs[2, 1].plot(rolling_win, label=\"Win\")\n",
    "    axs[2, 1].plot(rolling_draw, label=\"Draw\")\n",
    "    axs[2, 1].plot(rolling_loss, label=\"Loss\")\n",
    "    axs[2, 1].set_title(\"Rolling Win/Draw/Loss (500 ep)\")\n",
    "    axs[2, 1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Pretty Console Game ---\n",
    "def play_against_ai(model):\n",
    "    env = TicTacToeEnv()\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"\\nYou are 'O' and AI is 'X'. Enter your move as: row col (0-2)\\n\")\n",
    "\n",
    "    def display_board(board):\n",
    "        symbols = {0: ' ', 1: 'X', -1: 'O'}\n",
    "        for i in range(3):\n",
    "            row = ' | '.join(symbols[board[j]] for j in range(i*3, i*3+3))\n",
    "            print(f\" {row} \")\n",
    "            if i < 2:\n",
    "                print(\"-----------\")\n",
    "\n",
    "    def parse_move(inp):\n",
    "        try:\n",
    "            if len(inp.strip()) == 2 and inp[0].isdigit() and inp[1].isdigit():\n",
    "                row, col = int(inp[0]), int(inp[1])\n",
    "            else:\n",
    "                row, col = map(int, inp.strip().split())\n",
    "            if 0 <= row <= 2 and 0 <= col <= 2:\n",
    "                return row * 3 + col\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "    while not done:\n",
    "        display_board(env.board)\n",
    "        move_str = input(\"\\nYour move (row col): \")\n",
    "        move = parse_move(move_str)\n",
    "        if move is None or move not in env.get_valid_actions():\n",
    "            print(\"Invalid move. Try again.\")\n",
    "            continue\n",
    "\n",
    "        _, _, done = env.step(move, player=-1)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state_tensor = torch.FloatTensor(env.board).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state_tensor)\n",
    "            valid_q = q_values[0][env.get_valid_actions()]\n",
    "            ai_move = env.get_valid_actions()[valid_q.argmax().item()]\n",
    "        _, _, done = env.step(ai_move, player=1)\n",
    "\n",
    "    display_board(env.board)\n",
    "    print()\n",
    "    if env.winner == 1:\n",
    "        print(\"AI wins!\")\n",
    "    elif env.winner == -1:\n",
    "        print(\"You win!\")\n",
    "    else:\n",
    "        print(\"It's a draw!\")\n",
    "\n",
    "# --- Run Everything ---\n",
    "model = train_dqn()\n",
    "\n",
    "play_against_ai(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce4e5c-e2db-4cd9-b984-00baeb22d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
